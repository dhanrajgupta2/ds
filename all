EXP 1 : 

Data Wrangling-I
Perform the following operations using Python on any open-source dataset (e.g., data.csv)
1. Import all the required Python Libraries.
2. Locate an open source data from the web (e.g., https://www.kaggle.com). Provide a clear
description of the data and its source (i.e., URL of the web site).
3. Load the Dataset into pandas dataframe.
4. Data Pre-processing: check for missing values in the data using pandas isnull(), describe()
function to get some initial statistics. Provide variable descriptions. Types of variables etc. Check
the dimensions of the data frame.
5. Data Formatting and Data Normalization: Summarize the types of variables by checking the
data types (i.e., character, numeric, integer, factor, and logical) of the variables in the data set. If
variables are not in the correct data type, apply proper type conversions.
6. Turn categorical variables into quantitative variables in Python.
In addition to the codes and outputs, explain every operation that you do in the above steps and
explain everything that you do to import/read/scrape the data set.

solution :


# Step 1: Import Required Libraries
import pandas as pd
import numpy as np

# Step 2: Load the Dataset into a Pandas DataFrame
file_path = "a1.csv"  # Ensure the file is in the same directory or provide the full path
df = pd.read_csv(file_path)

# Step 3: Data Preprocessing
# Check for missing values
print("Missing Values in Each Column:")
print(df.isnull().sum())
print("\n")

# Get initial statistics
print("Basic Statistics:")
print(df.describe(include="all"))
print("\n")

# Check dimensions of the dataframe
print(f"Dataset Dimensions: {df.shape}")
print("\n")

# Check data types of each column
print("Data Types of Each Column:")
print(df.dtypes)
print("\n")

# Step 4: Data Formatting & Normalization
# Convert 'Value' column to numeric (handling errors if any non-numeric values exist)
df["Value"] = pd.to_numeric(df["Value"], errors="coerce")

# Display updated data types
print("Updated Data Types After Conversion:")
print(df.dtypes)
print("\n")

# Step 5: Convert Categorical Variables to Numeric (Encoding)
df_encoded = pd.get_dummies(df, columns=[
    "Industry_aggregation_NZSIOC",
    "Industry_code_NZSIOC",
    "Industry_name_NZSIOC",
    "Units",
    "Variable_code",
    "Variable_name",
    "Variable_category",
    "Industry_code_ANZSIC06"
], drop_first=True)  # drop_first=True to avoid multicollinearity

# Display the first few rows of the encoded dataset
print("Encoded DataFrame (First 5 Rows):")
print(df_encoded.head())

or 

# Step 1: Import the Required Python Libraries

import pandas as pd
import numpy as np
from sklearn.preprocessing import MinMaxScaler

# Step 3: Load the Dataset into a pandas DataFrame

url = "https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data"
column_names = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'species']
df = pd.read_csv(url, header=None, names=column_names)

# Step 4: Data Pre-processing

missing_values = df.isnull().sum() # missing values
missing_values

statistics = df.describe() # statistics
statistics

data_types = df.dtypes # data_types
data_types

dimensions = df.shape # dimensions
dimensions

# Step 5: Data Formatting and Normalization

# Ensure numerical columns are of type float
numerical_columns = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']
df[numerical_columns] = df[numerical_columns].astype(float)

# Ensure 'species' is of type category
df['species'] = df['species'].astype('category')

scaler = MinMaxScaler()
df[numerical_columns] = scaler.fit_transform(df[numerical_columns])

# Step 6: Convert Categorical Variables into Quantitative Variables

df_encoded = pd.get_dummies(df, columns=['species'])

<===========================================================================================================================================================================================================================================================>
EXP 2

Create an “Academic performance” dataset of students and perform the following operations using Python.
1. Scan all variables for missing values and inconsistencies. If there are missing values and/or inconsistencies,
use any of the suitable techniques to deal with them.
2. Scan all numeric variables for outliers. If there are outliers, use any of the suitable techniques to deal with
them.
3. Apply data transformations on at least one of the variables. The purpose of this transformation should be one
of the following reasons: to change the scale for better understanding of the variable, to convert a non- linear
relation into a linear one, or to decrease the skewness and convert the distribution into a normal distribution.

Solution:




import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
file_path = '/mnt/data/A2.csv'
df = pd.read_csv(file_path)

# 1. Scan all variables for missing values and inconsistencies
missing_values = df.isnull().sum()
missing_values

# Filling missing values with the mean of each subject
df['Subject 3'].fillna(df['Subject 3'].mean(), inplace=True)
df['Subject 4'].fillna(df['Subject 4'].mean(), inplace=True)

# Fixing negative attendance by replacing it with the median
median_attendance = df['Attendance'].median()
df['Attendance'] = df['Attendance'].apply(lambda x: median_attendance if x < 0 else x)
# or 
# median_attendance = df['Attendance'].median()
# df['Attendance'] = df['Attendance'].clip(lower=median_attendance)

# 2. Scan all numeric variables for outliers and handle them
# Detecting outliers using the IQR method
Q1 = df[['Subject 1', 'Subject 2', 'Subject 3', 'Subject 4', 'Attendance']].quantile(0.25)
Q3 = df[['Subject 1', 'Subject 2', 'Subject 3', 'Subject 4', 'Attendance']].quantile(0.75)
IQR = Q3 - Q1
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

# Capping outliers
for column in ['Subject 1', 'Subject 2', 'Subject 3', 'Subject 4', 'Attendance']:
    df[column] = np.where(df[column] < lower_bound[column], lower_bound[column], df[column])
    df[column] = np.where(df[column] > upper_bound[column], upper_bound[column], df[column])

# removing outliers

df = df[(df >= lower_bound) & (df <= upper_bound)].dropna()

# 3. Apply data transformation to reduce skewness
# Applying log transformation to 'Attendance' column
df['Log_Attendance'] = np.log(df['Attendance'] + 1)

# Visualizing the distribution before and after transformation
plt.figure(figsize=(14, 6))
plt.subplot(1, 2, 1)
sns.histplot(df['Attendance'], kde=True)
plt.title('Original Attendance Distribution')

plt.subplot(1, 2, 2)
sns.histplot(df['Log_Attendance'], kde=True)
plt.title('Log Transformed Attendance Distribution')

plt.show()


or ( without in-built function )


Step 1: Manually Checking & Handling Missing Values

import pandas as pd
import numpy as np

# Sample dataset creation
data = {
    'Student_ID': [1, 2, 3, 4, 5, 6],
    'Subject_1': [85, 90, np.nan, 75, 95, 100],  # Contains NaN
    'Subject_2': [78, 80, 82, np.nan, 88, 92],  # Contains NaN
    'Attendance': [95, 88, -5, 80, 100, 105]  # Contains an invalid negative value
}

df = pd.DataFrame(data)

# 1. Scanning for Missing Values Manually
missing_values = {}
for col in df.columns:
    count = 0
    for val in df[col]:
        if isinstance(val, float) and np.isnan(val):  # Check for NaN manually
            count += 1
    missing_values[col] = count

print("Missing values count per column:", missing_values)

# or 
# missing_values = {}
# for col in df.columns:
    # count = 0
    # for val in df[col]:
        # if type(val) == float and val != val:  # NaN check (since NaN != NaN)
            # count += 1
    # missing_values[col] = count

# print("Missing values count per column:", missing_values)

# Filling missing values manually (using column mean)
for col in ['Subject_1', 'Subject_2']:
    total = 0
    count = 0
    for val in df[col]:
        if not (isinstance(val, float) and np.isnan(val)):  # Ignore NaN values
            total += val
            count += 1

    mean_value = total / count  # Calculate mean
    for i in range(len(df[col])):
        if isinstance(df[col][i], float) and np.isnan(df[col][i]):  # If NaN, replace with mean
            df.at[i, col] = mean_value

# Fixing invalid attendance values (negative values)
for i in range(len(df['Attendance'])):
    if df.at[i, 'Attendance'] < 0:
        df.at[i, 'Attendance'] = 0  # Replace negative with 0
# or 
# for i in range(len(df['Attendance'])):  
    # if df['Attendance'][i] < 0:  
        # df.at[i, 'Attendance'] = 0 

Step 2: Detecting & Handling Outliers (IQR Method Without Built-in Functions)

# Manually finding Q1 and Q3
numeric_cols = ['Subject_1', 'Subject_2', 'Attendance']

for col in numeric_cols:
    sorted_vals = sorted(df[col])  # Sort values manually
    n = len(sorted_vals)
    
    q1_index = n // 4
    q3_index = (3 * n) // 4
    
    q1 = sorted_vals[q1_index]
    q3 = sorted_vals[q3_index]
    
    iqr = q3 - q1  # Interquartile Range
    lower_bound = q1 - 1.5 * iqr
    upper_bound = q3 + 1.5 * iqr

    # Manually replacing outliers with upper/lower bound
    for i in range(n):
        if sorted_vals[i] < lower_bound:
            df.at[i, col] = lower_bound
        elif sorted_vals[i] > upper_bound:
            df.at[i, col] = upper_bound

step 3 : Applying Manual Log Transformation (For Normalization)

# Log transformation (Manual log to reduce skewness)
df['Log_Attendance'] = [np.log(x + 1) for x in df['Attendance']]  # Adding 1 to avoid log(0)

<============================================================================================================================================================================================================================================================>

exp 3 

Descriptive Statistics - Measures of Central Tendency and variability perform the following operations on any
open source dataset (e.g., data.csv)
1.Provide summary statistics (mean, median, minimum, maximum, standard deviation) for a dataset (age,
income etc.) with numeric variables grouped by one of the qualitative (categorical) variables. For example,
if your categorical variable is age groups and quantitative variable is income, then provide summary
statistics of income grouped by the age groups. Create a list that contains a numeric value for each response
to the categorical variable.


2. Write a Python program to display some basic statistical details like percentile, mean, standard deviation
etc. of the species of ‘Iris-setosa’, ‘Iris-versicolor’ and ‘Iris-versicolor’ of iris.csv dataset.


answer : 

import pandas as pd
import numpy as np

# Task 1: Provide summary statistics for a dataset grouped by a categorical variable
data = pd.read_csv('data.csv')

# Assume 'Category' is the categorical variable and 'Value' is the numeric variable
grouped_stats = data.groupby('Category')['Value'].describe()
print("Summary Statistics by Category:")
print(grouped_stats)

# Compute median separately
grouped_median = data.groupby('Category')['Value'].median()
print("\nMedian by Category:")
print(grouped_median)

# Task 2: Load the Iris dataset
iris = pd.read_csv('iris.csv')

# Filter data for each species
setosa = iris[iris['species'] == 'Iris-setosa']
versicolor = iris[iris['species'] == 'Iris-versicolor']
virginica = iris[iris['species'] == 'Iris-virginica']

# Function to compute basic statistics
def basic_statistics(df, species_name):
    print(f"\nBasic Statistics for {species_name}:")
    print(df.describe())

# Display statistics separately for each species
print("Task 2: Iris Dataset Statistics")
basic_statistics(setosa, 'Iris-setosa')
basic_statistics(versicolor, 'Iris-versicolor')
basic_statistics(virginica, 'Iris-virginica')


<============================================================================================================================================================================================================================================================>
exp 4 :Create a Linear Regression Model using Python/R to predict home prices using Boston Housing Dataset(https://www.kaggle.com/c/boston-housing). 
The Boston Housing dataset contains information about various houses in Boston through different parameters.
There are 506 samples and 14 feature variables in this dataset.

sol: 

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

%matplotlib inline
import warnings
warnings.filterwarnings(action = 'ignore')

df = pd.read_csv("/home/student/Downloads/housing_data (1).csv")
df

name = ['CRIM','ZN','INDUS','CHAS','NOX','RM','AGE','DIS','RAD','TAX','PTRATIO','B','LSTAT','MEDV']

df.head()

df.tail()

df.shape

df.info()

df.isnull().sum()

for i in name:
    df[i].fillna(df[i].median(),inplace = True)
    
df.isnull().sum()

plt.figure(figsize = (12,12))
sns.heatmap(data = df.corr().round(2),annot = True,cmap = 'coolwarm',linewidths = 0.2,square = True)

df1 = df[['RM','TAX','PTRATIO','LSTAT','MEDV']]
df1.head()

sns.pairplot(df1)

d = df1.describe().round(2)
d

plt.figure(figsize = (20,3))

plt.subplot(1,2,1)
sns.boxplot(df1.MEDV,color = '#005030')
plt.title('Boxplot of MEDV')

plt.subplot(1,2,2)
sns.distplot(a = df1.MEDV,color = '#500050')
plt.title('Distribution Plot of MEDV')
plt.show()



Q3 = d['MEDV']['75%']
Q1 = d['MEDV']['25%']
IQR = Q3 - Q1
ub = Q3 + 1.5*IQR
lb = Q1 - 1.5*IQR

df1[df1['MEDV'] > ub]



print(f"Shape of the dataset before removing outlier {df1.shape} ")

df2 = df1[~(df1['MEDV'] == 50)]

print(f"Shape of the dataset after removing outlier {df2.shape} ")


plt.figure(figsize = (20,3))

plt.subplot(1,3,1)
sns.boxplot(df2['TAX'],color = '#005030')
plt.title('Boxplot of TAX')

plt.subplot(1,3,2)
sns.distplot(a = df2.TAX,color = '#500050')
plt.title('Distribution Plot of TAX')

plt.subplot(1,3,3)
sns.scatterplot(x = df2.TAX,y = df2.MEDV)
plt.title('Scatter Plot of TAX vs MEDV')

plt.show()    


temp_df = df2[df1['TAX'] > 600].sort_values(['RM','MEDV'])
temp_df.shape
temp_df.describe()



tax_10 = df2[(df2['TAX'] < 600) & (df2['LSTAT'] >= 0) & (df2['LSTAT'] < 10)]['TAX'].mean()
tax_20 = df2[(df2['TAX'] < 600) & (df2['LSTAT'] >= 10) & (df2['LSTAT'] < 20)]['TAX'].mean()
tax_30 = df2[(df2['TAX'] < 600) & (df2['LSTAT'] >= 20) & (df2['LSTAT'] < 30)]['TAX'].mean()
tax_40 = df2[(df2['TAX'] < 600) & (df2['LSTAT'] >= 30)]['TAX'].mean()

indexes = list(df2.index)
for i in indexes:
    if(0 <= df2['LSTAT'][i] < 10):
        df2.at[i,'TAX'] = tax_10
    elif(10 <= df2['LSTAT'][i] < 20):
        df2.at[i,'TAX'] = tax_20
    elif(20 <= df2['LSTAT'][i] < 30):
        df2.at[i,'TAX'] = tax_30
    elif(30 <= df2['LSTAT'][i]):
        df2.at[i,'TAX'] = tax_40
        
print('Value imputed successfully !')



df2[df2['TAX'] > 600]['TAX'].count()



sns.distplot(a = df2.TAX,color = '#500050')
plt.title('Distribution Plot of TAX after replacing extreme values')
plt.show()

df2.describe()








plt.figure(figsize = (20,3))

plt.subplot(1,3,1)
sns.boxplot(df2['PTRATIO'],color = '#005030')
plt.title('Boxplot of PTRATIO')

plt.subplot(1,3,2)
sns.distplot(a = df2.PTRATIO,color = '#500050')
plt.title('Distribution Plot of PTRATIO')

plt.subplot(1,3,3)
sns.scatterplot(x = df2.PTRATIO,y = df2.MEDV)
plt.title('Scatter Plot of PTRATIO vs MEDV')

plt.show()




plt.figure(figsize = (20,3))

plt.subplot(1,3,1)
sns.boxplot(df2['LSTAT'],color = '#005030')
plt.title('Boxplot of LSTAT')

plt.subplot(1,3,2)
sns.distplot(a = df2.LSTAT,color = '#500050')
plt.title('Distribution Plot of LSTAT')

plt.subplot(1,3,3)
sns.scatterplot(x = df2.LSTAT,y = df2.MEDV)
plt.title('Scatter Plot of LSTAT vs MEDV')

plt.show()





Q3 = d['LSTAT']['75%']
Q1 = d['LSTAT']['25%']
IQR = Q3 - Q1
ub = Q3 + 1.5*IQR
lb = Q1 - 1.5*IQR

df2[df2['LSTAT'] > ub].sort_values(by = 'LSTAT')




plt.figure(figsize = (20,3))

plt.subplot(1,3,1)
sns.boxplot(df2['RM'],color = '#005030')
plt.title('Boxplot of RM')

plt.subplot(1,3,2)
sns.distplot(a = df2.RM,color = '#500050')
plt.title('Distribution Plot of RM')

plt.subplot(1,3,3)
sns.scatterplot(x = df2.RM,y = df2.MEDV)
plt.title('Scatter Plot of RM vs MEDV')

plt.show()



Q3 = d['RM']['75%']
Q1 = d['RM']['25%']
IQR = Q3 - Q1
ub = Q3 + 1.5*IQR
lb = Q1 - 1.5*IQR

df2[df2['RM'] < lb].sort_values(by = ['RM','MEDV'])



print(f"Shape of the dataset before removing outlier {df2.shape} ")

df3 = df2.drop(axis = 0,index = [365,367])



print(f"Shape of the dataset after removing outlier {df3.shape} ")

df3[df3['RM'] > ub].sort_values(by = ['RM','MEDV'])




print(f"Shape of the dataset before removing outlier {df3.shape} ")

df3 = df3.drop(axis = 0,index = 364)


print(f"Shape of the dataset after removing outlier {df3.shape} ")


# now apply linear regression


#Now will split our dataset into Dependent variable and Independent variable

X = df3.iloc[:,0:4].values
y = df3.iloc[:,-1:].values



print(f"Shape of Dependent Variable X = {X.shape}")
print(f"Shape of Independent Variable y = {y.shape}")



def FeatureScaling(X):
    """
    is function takes an array as an input, which needs to be scaled down.
    Apply Standardization technique to it and scale down the features with mean = 0 and standard deviation = 1

    Input <- 2 dimensional numpy array
    Returns -> Numpy array after applying Feature Scaling
    """
    mean = np.mean(X,axis=0)
    std = np.std(X,axis=0)
    for i in range(X.shape[1]):
        X[:,i] = (X[:,i]-mean[i])/std[i]
    return X



X = FeatureScaling(X)



m,n = X.shape
X = np.append(arr=np.ones((m,1)),values=X,axis=1)



#Now we will spit our data into Train set and Test Set

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state = 42)

print(f"Shape of X_train = {X_train.shape}")
print(f"Shape of X_test = {X_test.shape}")
print(f"Shape of y_train = {y_train.shape}")
print(f"Shape of y_test = {y_test.shape}")



#ComputeCost function determines the cost (sum of squared errors)

def ComputeCost(X,y,theta):
    """
    This function takes three inputs and uses the Cost Function to determine the cost (basically error of prediction vs
    actual values)
    Cost Function: Sum of square of error in predicted values divided by number of data points in the set
    J = 1/(2*m) *  Summation(Square(Predicted values - Actual values))

    Input <- Take three numoy array X,y and theta
    Return -> The cost calculated from the Cost Function
    """
    m=X.shape[0] #number of data points in the set
    J = (1/(2*m)) * np.sum((X.dot(theta) - y)**2)
    return J



#Gradient Descent Algorithm to minimize the Cost and find best parameters in order to get best line for our dataset

def GradientDescent(X,y,theta,alpha,no_of_iters):
    m=X.shape[0]
    J_Cost = []
    for i in range(no_of_iters):
        error = np.dot(X.transpose(),(X.dot(theta)-y))
        theta = theta - alpha * (1/m) * error
        J_Cost.append(ComputeCost(X,y,theta))

    return theta, np.array(J_Cost)



iters = 1000

alpha1 = 0.001
theta1 = np.zeros((X_train.shape[1],1))
theta1, J_Costs1 = GradientDescent(X_train,y_train,theta1,alpha1,iters)

alpha2 = 0.003
theta2 = np.zeros((X_train.shape[1],1))
theta2, J_Costs2 = GradientDescent(X_train,y_train,theta2,alpha2,iters)

alpha3 = 0.01
theta3 = np.zeros((X_train.shape[1],1))
theta3, J_Costs3 = GradientDescent(X_train,y_train,theta3,alpha3,iters)

alpha4 = 0.03
theta4 = np.zeros((X_train.shape[1],1))
theta4, J_Costs4 = GradientDescent(X_train,y_train,theta4,alpha4,iters)



plt.figure(figsize=(8,5))
plt.plot(J_Costs1,label = 'alpha = 0.001')
plt.plot(J_Costs2,label = 'alpha = 0.003')
plt.plot(J_Costs3,label = 'alpha = 0.01')
plt.plot(J_Costs4,label = 'alpha = 0.03')
plt.title('Convergence of Gradient Descent for different values of alpha')
plt.xlabel('No. of iterations')
plt.ylabel('Cost')
plt.legend()
plt.show()



theta4



def Predict(X,theta):
    """
    This function predicts the result for the unseen data
    """
    y_pred = X.dot(theta)
    return y_pred



y_pred = Predict(X_test,theta4)
y_pred[:5]



plt.scatter(x=y_test,y=y_pred,alpha=0.5)
plt.xlabel('y_test',size=12)
plt.ylabel('y_pred',size=12)
plt.title('Predicited Values vs Original Values (Test Set)',size=15)
plt.show()




sns.residplot(x = y_pred,y = (y_pred-y_test))
plt.xlabel('Predicited Values',size=12)
plt.ylabel("Residues",size=12)
plt.title('Residual Plot',size=15)
plt.show()




sns.distplot(y_pred-y_test)
plt.xlabel('Residual',size=12)
plt.ylabel('Frquency',size=12)
plt.title('Distribution of Residuals',size=15)
plt.show()



from sklearn import metrics
r2= metrics.r2_score(y_test,y_pred)
N,p = X_test.shape
adj_r2 = 1-((1-r2)*(N-1))/(N-p-1)
print(f'R^2 = {r2}')
print(f'Adjusted R^2 = {adj_r2}')



from sklearn import metrics
mse = metrics.mean_squared_error(y_test,y_pred)
mae = metrics.mean_absolute_error(y_test,y_pred)
rmse = np.sqrt(metrics.mean_squared_error(y_test,y_pred))
print(f'Mean Squared Error: {mse}',f'Mean Absolute Error: {mae}',f'Root Mean Squared Error: {rmse}',sep='\n')



#coefficients of regression model
coeff=np.array([y for x in theta4 for y in x]).round(2)
features=['Bias','RM','TAX','PTRATIO','LSTAT']
eqn = 'MEDV = '
for f,c in zip(features,coeff):
    eqn+=f" + ({c} * {f})";

print(eqn)



sns.barplot(x=features,y=coeff)
plt.ylim([-5,25])
plt.xlabel('Coefficient Names',size=12)
plt.ylabel('Coefficient Values',size=12)
plt.title('Visualising Regression Coefficients',size=15)
plt.show()

or


import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error

# Load the dataset
df = pd.read_csv('/mnt/data/housing_data.csv')

# Define features and target variable
X = df.drop(columns=['MEDV'])  # 'MEDV' is assumed to be the target variable
y = df['MEDV']

# Split the dataset into training (80%) and testing (20%) sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create and train the Linear Regression model
model = LinearRegression()
model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = model.predict(X_test)

# Evaluate the model using Mean Squared Error (MSE)
mse = mean_squared_error(y_test, y_pred)
print(f'Mean Squared Error: {mse:.2f}')

# Visualize actual vs predicted prices
plt.figure(figsize=(8, 6))
plt.scatter(y_test, y_pred, alpha=0.7, color='blue')
plt.xlabel("Actual Prices")
plt.ylabel("Predicted Prices")
plt.title("Actual vs Predicted Prices")
plt.grid(True)
plt.show()



<============================================================================================================================================================================================================================================================>
exp 5 : Data Analytics II
1. Implement logistic regression using Python/R to perform classification on
Social_Network_Ads.csv dataset.
2. Compute Confusion matrix to find TP, FP, TN, FN, Accuracy, Error rate, Precision, Recall
on the given dataset

sol:

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression

file_path = "/home/student/Downloads/Social_Network_Ads (1).csv"
df = pd.read_csv(file_path)
df

df.head(10) # represents first 10 only

X = df[["Age", "EstimatedSalary"]]
Y = df[["Purchased"]]

x_train, x_test, y_train, y_test = train_test_split(X,Y, train_size = 0.8, random_state = 10 )

model = LogisticRegression()
model.fit(x_train,y_train)

y_predicted = model.predict(x_test)
y_predicted

model.score(x_test,y_test)

from sklearn.metrics import confusion_matrix,accuracy_score,precision_score,recall_score,f1_score

confusion_matrix(y_test,y_predicted)

accuracy = accuracy_score(y_test,y_predicted)

print("accuracy", accuracy)

precision = precision_score(y_test,y_predicted)
print("precision", precision)

recall = recall_score(y_test,y_predicted)
print("recall", recall)

f1 = f1_score(y_test,y_predicted)
print("f1", f1)

<============================================================================================================================================================================================================================================================>
Exp 6 

1.Implement Simple Naïve Bayes classification algorithm using Python/R on iris.csv dataset.

2. Compute Confusion matrix to find TP, FP, TN, FN, Accuracy, Error rate, Precision, Recall on
the given dataset.

Answer 


import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score

# Load the Iris dataset
data = pd.read_csv("iris.csv")

# Display first few rows
print(data.head())

# Check data distribution
print(data['species'].value_counts())

# Splitting the dataset into features and target variable
X = data.iloc[:, :-1]  # Features (sepal length, width, petal length, width)
y = data.iloc[:, -1]   # Target variable (species)

# Splitting into train and test sets (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train Naïve Bayes classifier
model = GaussianNB()
model.fit(X_train, y_train)

# Make predictions on test data
y_pred = model.predict(X_test)

# Compute Confusion Matrix
conf_matrix = confusion_matrix(y_test, y_pred)
print("Confusion Matrix:\n", conf_matrix)

# Extract TP, FP, TN, FN for each class
TP = np.diag(conf_matrix)  # True Positives
FP = conf_matrix.sum(axis=0) - TP  # False Positives
FN = conf_matrix.sum(axis=1) - TP  # False Negatives
TN = conf_matrix.sum() - (TP + FP + FN)  # True Negatives

# Compute performance metrics
accuracy = accuracy_score(y_test, y_pred)
error_rate = 1 - accuracy
precision = TP / (TP + FP)
recall = TP / (TP + FN)

# Display metrics
print(f"Accuracy: {accuracy:.4f}")
print(f"Error Rate: {error_rate:.4f}")
print(f"Precision per class: {precision}")
print(f"Recall per class: {recall}")

# Classification Report
print("Classification Report:\n", classification_report(y_test, y_pred))

# Plot confusion matrix heatmap
plt.figure(figsize=(6, 5))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=data['species'].unique(), yticklabels=data['species'].unique())
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix Heatmap")
plt.show()

<============================================================================================================================================================================================================================================================>
EXP 8 

Data Visualization II
1. Use the inbuilt dataset 'titanic' as used in the above problem. Plot a box plot for distribution of age
with respect to each gender along with the information about whether they survived or not. (Column
names : 'sex' and 'age')
2. Write observations on the inference from the above statistics.

answer 


import seaborn as sns
import pandas as pd
import matplotlib.pyplot as plt

# Load Titanic dataset
data = sns.load_dataset('titanic')



# Boxplot for Age vs. Survival
plt.figure(figsize=(10, 6))
sns.boxplot(x='survived', y='age', hue='sex', data=data, palette='coolwarm')
plt.title('Age Distribution by Survival and Gender')
plt.xlabel('Survived')
plt.ylabel('Age')
plt.show()

# Boxplot for Fare vs. Survival
plt.figure(figsize=(10, 6))
sns.boxplot(x='survived', y='fare', hue='sex', data=data, palette='coolwarm')
plt.title('Fare Distribution by Survival and Gender')
plt.xlabel('Survived')
plt.ylabel('Fare')
plt.show()

# Boxplot for Pclass vs. Survival
plt.figure(figsize=(10, 6))
sns.boxplot(x='survived', y='pclass', hue='sex', data=data, palette='coolwarm')
plt.title('Passenger Class Distribution by Survival and Gender')
plt.xlabel('Survived')
plt.ylabel('Pclass')
plt.show()

# Boxplot for SibSp vs. Survival
plt.figure(figsize=(10, 6))
sns.boxplot(x='survived', y='sibsp', hue='sex', data=data, palette='coolwarm')
plt.title('Siblings/Spouses Aboard vs. Survival and Gender')
plt.xlabel('Survived')
plt.ylabel('SibSp')
plt.show()

# Boxplot for Parch vs. Survival
plt.figure(figsize=(10, 6))
sns.boxplot(x='survived', y='parch', hue='sex', data=data, palette='coolwarm')
plt.title('Parents/Children Aboard vs. Survival and Gender')
plt.xlabel('Survived')
plt.ylabel('Parch')
plt.show()

<============================================================================================================================================================================================================================================================>


EXP 9 

Data Visualization II
1. Use the inbuilt dataset 'titanic' as used in the above problem. Plot a box plot for distribution of age
with respect to each gender along with the information about whether they survived or not. (Column
names : 'sex' and 'age')
2. Write observations on the inference from the above statistics.

answers


import seaborn as sns
import pandas as pd
import matplotlib.pyplot as plt

# Load Titanic dataset from Seaborn's built-in datasets
data = sns.load_dataset('titanic')

# Display first few rows
print(data.head())

# Check for missing values
data.isnull().sum()

# Drop rows with missing values for simplicity
data.dropna(inplace=True)

# Countplot for survival distribution
plt.figure(figsize=(8, 5))
sns.countplot(x='survived', data=data, palette='coolwarm')
plt.title('Survival Count')
plt.show()

# Countplot for gender vs survival
plt.figure(figsize=(8, 5))
sns.countplot(x='sex', hue='survived', data=data, palette='coolwarm')
plt.title('Survival Count by Gender')
plt.show()

# Countplot for class vs survival
plt.figure(figsize=(8, 5))
sns.countplot(x='class', hue='survived', data=data, palette='coolwarm')
plt.title('Survival Count by Class')
plt.show()

# Histogram for fare distribution
plt.figure(figsize=(10, 5))
sns.histplot(data['fare'], bins=30, kde=True, color='green')
plt.title('Fare Distribution')
plt.xlabel('Fare')
plt.ylabel('Frequency')
plt.show()

<============================================================================================================================================================================================================================================================>
