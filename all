EXP 1 : 

Data Wrangling-I
Perform the following operations using Python on any open-source dataset (e.g., data.csv)
1. Import all the required Python Libraries.
2. Locate an open source data from the web (e.g., https://www.kaggle.com). Provide a clear
description of the data and its source (i.e., URL of the web site).
3. Load the Dataset into pandas dataframe.
4. Data Pre-processing: check for missing values in the data using pandas isnull(), describe()
function to get some initial statistics. Provide variable descriptions. Types of variables etc. Check
the dimensions of the data frame.
5. Data Formatting and Data Normalization: Summarize the types of variables by checking the
data types (i.e., character, numeric, integer, factor, and logical) of the variables in the data set. If
variables are not in the correct data type, apply proper type conversions.
6. Turn categorical variables into quantitative variables in Python.
In addition to the codes and outputs, explain every operation that you do in the above steps and
explain everything that you do to import/read/scrape the data set.

solution :


# Step 1: Import Required Libraries
import pandas as pd
import numpy as np

# Step 2: Load the Dataset into a Pandas DataFrame
file_path = "a1.csv"  # Ensure the file is in the same directory or provide the full path
df = pd.read_csv(file_path)

# Step 3: Data Preprocessing
# Check for missing values
print("Missing Values in Each Column:")
print(df.isnull().sum())
print("\n")

# Get initial statistics
print("Basic Statistics:")
print(df.describe(include="all"))
print("\n")

# Check dimensions of the dataframe
print(f"Dataset Dimensions: {df.shape}")
print("\n")

# Check data types of each column
print("Data Types of Each Column:")
print(df.dtypes)
print("\n")

# Step 4: Data Formatting & Normalization
# Convert 'Value' column to numeric (handling errors if any non-numeric values exist)
df["Value"] = pd.to_numeric(df["Value"], errors="coerce")

# Display updated data types
print("Updated Data Types After Conversion:")
print(df.dtypes)
print("\n")

# Step 5: Convert Categorical Variables to Numeric (Encoding)
df_encoded = pd.get_dummies(df, columns=[
    "Industry_aggregation_NZSIOC",
    "Industry_code_NZSIOC",
    "Industry_name_NZSIOC",
    "Units",
    "Variable_code",
    "Variable_name",
    "Variable_category",
    "Industry_code_ANZSIC06"
], drop_first=True)  # drop_first=True to avoid multicollinearity

# Display the first few rows of the encoded dataset
print("Encoded DataFrame (First 5 Rows):")
print(df_encoded.head())

or 

# Step 1: Import the Required Python Libraries

import pandas as pd
import numpy as np
from sklearn.preprocessing import MinMaxScaler

# Step 3: Load the Dataset into a pandas DataFrame

url = "https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data"
column_names = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'species']
df = pd.read_csv(url, header=None, names=column_names)

# Step 4: Data Pre-processing

missing_values = df.isnull().sum() # missing values
missing_values

statistics = df.describe() # statistics
statistics

data_types = df.dtypes # data_types
data_types

dimensions = df.shape # dimensions
dimensions

# Step 5: Data Formatting and Normalization

# Ensure numerical columns are of type float
numerical_columns = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']
df[numerical_columns] = df[numerical_columns].astype(float)

# Ensure 'species' is of type category
df['species'] = df['species'].astype('category')

scaler = MinMaxScaler()
df[numerical_columns] = scaler.fit_transform(df[numerical_columns])

# Step 6: Convert Categorical Variables into Quantitative Variables

df_encoded = pd.get_dummies(df, columns=['species'])

<===========================================================================================================================================================================================================================================================>
EXP 2

Create an “Academic performance” dataset of students and perform the following operations using Python.
1. Scan all variables for missing values and inconsistencies. If there are missing values and/or inconsistencies,
use any of the suitable techniques to deal with them.
2. Scan all numeric variables for outliers. If there are outliers, use any of the suitable techniques to deal with
them.
3. Apply data transformations on at least one of the variables. The purpose of this transformation should be one
of the following reasons: to change the scale for better understanding of the variable, to convert a non- linear
relation into a linear one, or to decrease the skewness and convert the distribution into a normal distribution.

Solution:




import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
file_path = '/mnt/data/A2.csv'
df = pd.read_csv(file_path)

# 1. Scan all variables for missing values and inconsistencies
missing_values = df.isnull().sum()
missing_values

# Filling missing values with the mean of each subject
df['Subject 3'].fillna(df['Subject 3'].mean(), inplace=True)
df['Subject 4'].fillna(df['Subject 4'].mean(), inplace=True)

# Fixing negative attendance by replacing it with the median
median_attendance = df['Attendance'].median()
df['Attendance'] = df['Attendance'].apply(lambda x: median_attendance if x < 0 else x)
# or 
# median_attendance = df['Attendance'].median()
# df['Attendance'] = df['Attendance'].clip(lower=median_attendance)

# 2. Scan all numeric variables for outliers and handle them
# Detecting outliers using the IQR method
Q1 = df[['Subject 1', 'Subject 2', 'Subject 3', 'Subject 4', 'Attendance']].quantile(0.25)
Q3 = df[['Subject 1', 'Subject 2', 'Subject 3', 'Subject 4', 'Attendance']].quantile(0.75)
IQR = Q3 - Q1
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

# Capping outliers
for column in ['Subject 1', 'Subject 2', 'Subject 3', 'Subject 4', 'Attendance']:
    df[column] = np.where(df[column] < lower_bound[column], lower_bound[column], df[column])
    df[column] = np.where(df[column] > upper_bound[column], upper_bound[column], df[column])

# removing outliers

df = df[(df >= lower_bound) & (df <= upper_bound)].dropna()

# 3. Apply data transformation to reduce skewness
# Applying log transformation to 'Attendance' column
df['Log_Attendance'] = np.log(df['Attendance'] + 1)

# Visualizing the distribution before and after transformation
plt.figure(figsize=(14, 6))
plt.subplot(1, 2, 1)
sns.histplot(df['Attendance'], kde=True)
plt.title('Original Attendance Distribution')

plt.subplot(1, 2, 2)
sns.histplot(df['Log_Attendance'], kde=True)
plt.title('Log Transformed Attendance Distribution')

plt.show()


or ( without in-built function )


Step 1: Manually Checking & Handling Missing Values

import pandas as pd
import numpy as np

# Sample dataset creation
data = {
    'Student_ID': [1, 2, 3, 4, 5, 6],
    'Subject_1': [85, 90, np.nan, 75, 95, 100],  # Contains NaN
    'Subject_2': [78, 80, 82, np.nan, 88, 92],  # Contains NaN
    'Attendance': [95, 88, -5, 80, 100, 105]  # Contains an invalid negative value
}

df = pd.DataFrame(data)

# 1. Scanning for Missing Values Manually
missing_values = {}
for col in df.columns:
    count = 0
    for val in df[col]:
        if isinstance(val, float) and np.isnan(val):  # Check for NaN manually
            count += 1
    missing_values[col] = count

print("Missing values count per column:", missing_values)

# or 
# missing_values = {}
# for col in df.columns:
    # count = 0
    # for val in df[col]:
        # if type(val) == float and val != val:  # NaN check (since NaN != NaN)
            # count += 1
    # missing_values[col] = count

# print("Missing values count per column:", missing_values)

# Filling missing values manually (using column mean)
for col in ['Subject_1', 'Subject_2']:
    total = 0
    count = 0
    for val in df[col]:
        if not (isinstance(val, float) and np.isnan(val)):  # Ignore NaN values
            total += val
            count += 1

    mean_value = total / count  # Calculate mean
    for i in range(len(df[col])):
        if isinstance(df[col][i], float) and np.isnan(df[col][i]):  # If NaN, replace with mean
            df.at[i, col] = mean_value

# Fixing invalid attendance values (negative values)
for i in range(len(df['Attendance'])):
    if df.at[i, 'Attendance'] < 0:
        df.at[i, 'Attendance'] = 0  # Replace negative with 0
# or 
# for i in range(len(df['Attendance'])):  
    # if df['Attendance'][i] < 0:  
        # df.at[i, 'Attendance'] = 0 

Step 2: Detecting & Handling Outliers (IQR Method Without Built-in Functions)

# Manually finding Q1 and Q3
numeric_cols = ['Subject_1', 'Subject_2', 'Attendance']

for col in numeric_cols:
    sorted_vals = sorted(df[col])  # Sort values manually
    n = len(sorted_vals)
    
    q1_index = n // 4
    q3_index = (3 * n) // 4
    
    q1 = sorted_vals[q1_index]
    q3 = sorted_vals[q3_index]
    
    iqr = q3 - q1  # Interquartile Range
    lower_bound = q1 - 1.5 * iqr
    upper_bound = q3 + 1.5 * iqr

    # Manually replacing outliers with upper/lower bound
    for i in range(n):
        if sorted_vals[i] < lower_bound:
            df.at[i, col] = lower_bound
        elif sorted_vals[i] > upper_bound:
            df.at[i, col] = upper_bound

step 3 : Applying Manual Log Transformation (For Normalization)

# Log transformation (Manual log to reduce skewness)
df['Log_Attendance'] = [np.log(x + 1) for x in df['Attendance']]  # Adding 1 to avoid log(0)

<============================================================================================================================================================================================================================================================>

exp 5 : Data Analytics II
1. Implement logistic regression using Python/R to perform classification on
Social_Network_Ads.csv dataset.
2. Compute Confusion matrix to find TP, FP, TN, FN, Accuracy, Error rate, Precision, Recall
on the given dataset

sol:

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression

file_path = "/home/student/Downloads/Social_Network_Ads (1).csv"
df = pd.read_csv(file_path)
df

df.head(10) # represents first 10 only

X = df[["Age", "EstimatedSalary"]]
Y = df[["Purchased"]]

x_train, x_test, y_train, y_test = train_test_split(X,Y, train_size = 0.8, random_state = 10 )

model = LogisticRegression()
model.fit(x_train,y_train)

y_predicted = model.predict(x_test)
y_predicted

model.score(x_test,y_test)

from sklearn.metrics import confusion_matrix,accuracy_score,precision_score,recall_score,f1_score

confusion_matrix(y_test,y_predicted)

accuracy = accuracy_score(y_test,y_predicted)

print("accuracy", accuracy)

precision = precision_score(y_test,y_predicted)
print("precision", precision)

recall = recall_score(y_test,y_predicted)
print("recall", recall)

f1 = f1_score(y_test,y_predicted)
print("f1", f1)

<============================================================================================================================================================================================================================================================>

EXP 8 

""""Data Visualization I
1. Use the inbuilt dataset 'titanic'. The dataset contains 891 rows and contains information
about the passengers who boarded the unfortunate Titanic ship. Use the Seaborn library to
see if we can find any patterns in the data.
2. Write a code to check how the price of the ticket (column name: 'fare') for each passenger
is distributed by plotting a histogram."""

answer


import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import warnings
warnings.filterwarnings('ignore')
data=pd.read_csv('/home/student/Downloads/Titanic/Titanic-Dataset.csv')
data


data.head()

data.shape

data.get

data.sample(891)

data.isnull().sum()


print(data.drop(['Cabin'],axis=1,inplace=True,errors='ignore'))


data["Age"].fillna(data["Age"].mean(),inplace=True)


print(data["Embarked"].fillna("S",inplace=True))
data.isnull().sum()


#data analysis
data.describe()


import seaborn as sns
data["Survived"].value_counts()


sns.countplot(x="Survived",data=data)
#plot os count


sns.countplot(x="Sex", hue="Survived", data=data)

sns.countplot(x="Pclass", hue="Survived", data=data)

sns.countplot(x="Embarked", hue="Survived", data=data)

sns.countplot(x="Parch", hue="Survived", data=data)

sns.countplot(x="SibSp", hue="Survived", data=data)

sns.countplot(x="Age", hue="Survived", data=data)

sns.countplot(x="Ticket", hue="Survived", data=data)


sns.countplot(x="Fare", hue="Survived", data=data)

sns.countplot(x="PassengerId", hue="Survived", data=data)


import matplotlib.pyplot as plt
plt.figure(figsize=(12, 6))
plt.subplot(1, 2, 1)
sns.histplot(data['Age'].dropna(), kde=True, bins=20, color='pink')
plt.title('Histogram of Age')
plt.xlabel('Age')
plt.ylabel('Frequency')
plt.subplot(1, 2, 2)
sns.histplot(data['Fare'].dropna(), kde=True, bins=20, color='green')
plt.title('Histogram of Fare')
plt.xlabel('Fare')
plt.ylabel('Frequency')
plt.tight_layout()
plt.show()

or 

# Import necessary libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings

# Ignore warnings for clean output
warnings.filterwarnings('ignore')

# Load Titanic dataset
data = pd.read_csv('/home/student/Downloads/Titanic/Titanic-Dataset.csv')

# Display dataset
data.head()

# Check shape of dataset
print("Shape of dataset:", data.shape)

# Check missing values
print("Missing values before cleaning:\n", data.isnull().sum())

# Drop 'Cabin' column due to too many missing values
data.drop(['Cabin'], axis=1, inplace=True, errors='ignore')

# Fill missing Age values with mean age
data["Age"].fillna(data["Age"].mean(), inplace=True)

# Fill missing Embarked values with most common value 'S'
data["Embarked"].fillna("S", inplace=True)

# Check missing values after cleaning
print("Missing values after cleaning:\n", data.isnull().sum())

# Data Summary
print("Dataset Summary:\n", data.describe())

# Count of passengers survived vs. not survived
print("Survival Count:\n", data["Survived"].value_counts())

# Data Visualization
sns.set_style("whitegrid")

# Count plots for categorical features
plt.figure(figsize=(15, 8))

plt.subplot(2, 3, 1)
sns.countplot(x="Survived", data=data)
plt.title("Survival Count")

plt.subplot(2, 3, 2)
sns.countplot(x="Sex", hue="Survived", data=data)
plt.title("Survival by Gender")

plt.subplot(2, 3, 3)
sns.countplot(x="Pclass", hue="Survived", data=data)
plt.title("Survival by Passenger Class")

plt.subplot(2, 3, 4)
sns.countplot(x="Embarked", hue="Survived", data=data)
plt.title("Survival by Embarkation Point")

plt.subplot(2, 3, 5)
sns.countplot(x="Parch", hue="Survived", data=data)
plt.title("Survival by Number of Parents/Children")

plt.subplot(2, 3, 6)
sns.countplot(x="SibSp", hue="Survived", data=data)
plt.title("Survival by Number of Siblings/Spouses")

plt.tight_layout()
plt.show()

# Histogram plots
plt.figure(figsize=(12, 6))

# Histogram of Age
plt.subplot(1, 2, 1)
sns.histplot(data['Age'].dropna(), kde=True, bins=20, color='pink')
plt.title('Histogram of Age')
plt.xlabel('Age')
plt.ylabel('Frequency')

# Histogram of Fare
plt.subplot(1, 2, 2)
sns.histplot(data['Fare'].dropna(), kde=True, bins=20, color='green')
plt.title('Histogram of Fare')
plt.xlabel('Fare')
plt.ylabel('Frequency')

plt.tight_layout()
plt.show()

# Histogram of Fare with Log Transformation (For Better Visualization)
plt.figure(figsize=(8, 5))
sns.histplot(np.log1p(data['Fare']), kde=True, bins=20, color='blue')
plt.title('Histogram of Log(Fare)')
plt.xlabel('Log(Fare)')
plt.ylabel('Frequency')
plt.show()

<============================================================================================================================================================================================================================================================>


