EXP 2:

Create an “Academic performance” dataset of students and perform the following operations using Python.
1. Scan all variables for missing values and inconsistencies. If there are missing values and/or inconsistencies,
use any of the suitable techniques to deal with them.
2. Scan all numeric variables for outliers. If there are outliers, use any of the suitable techniques to deal with
them.
3. Apply data transformations on at least one of the variables. The purpose of this transformation should be one
of the following reasons: to change the scale for better understanding of the variable, to convert a non- linear
relation into a linear one, or to decrease the skewness and convert the distribution into a normal distribution.

Solution:




import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
file_path = '/mnt/data/A2.csv'
df = pd.read_csv(file_path)

# 1. Scan all variables for missing values and inconsistencies
missing_values = df.isnull().sum()
missing_values

# Filling missing values with the mean of each subject
df['Subject 3'].fillna(df['Subject 3'].mean(), inplace=True)
df['Subject 4'].fillna(df['Subject 4'].mean(), inplace=True)

# Fixing negative attendance by replacing it with the median
median_attendance = df['Attendance'].median()
df['Attendance'] = df['Attendance'].apply(lambda x: median_attendance if x < 0 else x)
# or 
# median_attendance = df['Attendance'].median()
# df['Attendance'] = df['Attendance'].clip(lower=median_attendance)


# 2. Scan all numeric variables for outliers and handle them
# Detecting outliers using the IQR method
Q1 = df[['Subject 1', 'Subject 2', 'Subject 3', 'Subject 4', 'Attendance']].quantile(0.25)
Q3 = df[['Subject 1', 'Subject 2', 'Subject 3', 'Subject 4', 'Attendance']].quantile(0.75)
IQR = Q3 - Q1
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

# Capping outliers
for column in ['Subject 1', 'Subject 2', 'Subject 3', 'Subject 4', 'Attendance']:
    df[column] = np.where(df[column] < lower_bound[column], lower_bound[column], df[column])
    df[column] = np.where(df[column] > upper_bound[column], upper_bound[column], df[column])

# removing outliers

df = df[(df >= lower_bound) & (df <= upper_bound)].dropna()


# 3. Apply data transformation to reduce skewness
# Applying log transformation to 'Attendance' column
df['Log_Attendance'] = np.log(df['Attendance'] + 1)

# Visualizing the distribution before and after transformation
plt.figure(figsize=(14, 6))
plt.subplot(1, 2, 1)
sns.histplot(df['Attendance'], kde=True)
plt.title('Original Attendance Distribution')

plt.subplot(1, 2, 2)
sns.histplot(df['Log_Attendance'], kde=True)
plt.title('Log Transformed Attendance Distribution')

plt.show()


or ( without in-built function )


Step 1: Manually Checking & Handling Missing Values

import pandas as pd
import numpy as np

# Sample dataset creation
data = {
    'Student_ID': [1, 2, 3, 4, 5, 6],
    'Subject_1': [85, 90, np.nan, 75, 95, 100],  # Contains NaN
    'Subject_2': [78, 80, 82, np.nan, 88, 92],  # Contains NaN
    'Attendance': [95, 88, -5, 80, 100, 105]  # Contains an invalid negative value
}

df = pd.DataFrame(data)

# 1. Scanning for Missing Values Manually
missing_values = {}
for col in df.columns:
    count = 0
    for val in df[col]:
        if isinstance(val, float) and np.isnan(val):  # Check for NaN manually
            count += 1
    missing_values[col] = count

print("Missing values count per column:", missing_values)

# or 
# missing_values = {}
# for col in df.columns:
    # count = 0
    # for val in df[col]:
        # if type(val) == float and val != val:  # NaN check (since NaN != NaN)
            # count += 1
    # missing_values[col] = count

# print("Missing values count per column:", missing_values)




# Filling missing values manually (using column mean)
for col in ['Subject_1', 'Subject_2']:
    total = 0
    count = 0
    for val in df[col]:
        if not (isinstance(val, float) and np.isnan(val)):  # Ignore NaN values
            total += val
            count += 1

    mean_value = total / count  # Calculate mean
    for i in range(len(df[col])):
        if isinstance(df[col][i], float) and np.isnan(df[col][i]):  # If NaN, replace with mean
            df.at[i, col] = mean_value

# Fixing invalid attendance values (negative values)
for i in range(len(df['Attendance'])):
    if df.at[i, 'Attendance'] < 0:
        df.at[i, 'Attendance'] = 0  # Replace negative with 0

Step 2: Detecting & Handling Outliers (IQR Method Without Built-in Functions)

# Manually finding Q1 and Q3
numeric_cols = ['Subject_1', 'Subject_2', 'Attendance']

for col in numeric_cols:
    sorted_vals = sorted(df[col])  # Sort values manually
    n = len(sorted_vals)
    
    q1_index = n // 4
    q3_index = (3 * n) // 4
    
    q1 = sorted_vals[q1_index]
    q3 = sorted_vals[q3_index]
    
    iqr = q3 - q1  # Interquartile Range
    lower_bound = q1 - 1.5 * iqr
    upper_bound = q3 + 1.5 * iqr

    # Manually replacing outliers with upper/lower bound
    for i in range(n):
        if sorted_vals[i] < lower_bound:
            df.at[i, col] = lower_bound
        elif sorted_vals[i] > upper_bound:
            df.at[i, col] = upper_bound

step 3 : Applying Manual Log Transformation (For Normalization)

# Log transformation (Manual log to reduce skewness)
df['Log_Attendance'] = [np.log(x + 1) for x in df['Attendance']]  # Adding 1 to avoid log(0)
